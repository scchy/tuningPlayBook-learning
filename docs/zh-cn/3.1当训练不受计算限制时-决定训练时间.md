## 当训练不受计算限制时-决定训练时间

1. 我们的主要目标是确保我们训练的时间足够长，以使模型达到最佳结果，同时<font color=darkred>避免在训练步骤的数量上过度浪费</font>。
2. 当有疑问时，在训练的时间上犯错误。如果正确使用了回溯（最佳）检查点选择，并且检查点足够频繁，那么训练时间越长，性能就不会下降。
3. 切勿调整研究中的`max_train_steps`数。选择一个值并将其用于所有试验。从这些试验中，绘制回顾性检查点选择找到的训练步骤，以优化`max_train_steps`的选择。
   - 例如，如果最佳步数总是在训练的前10%，那么最大步数太高了。
   - 或者，如果最好的步骤始终在训练的最后25%，我们可能会从更长时间的训练和重新调整衰减时间表中受益。
4. 当体系结构或数据发生变化时（例如增加数据扩充），理想的训练步骤数可能会发生变化。
5. 下面我们将描述如何根据使用恒定学习率“完美匹配”训练集所需的步骤数来选择`max_train_steps`的初始候选值
    1. 注意，我们没有以精确或数学上定义良好的方式使用短语“完美匹配训练集”。这仅仅是一个非正式的描述，表示训练损失很低。
        - 例如，当使用对数损失进行训练时，如果没有正则化项，我们可能会看到训练损失一直在缓慢改善，直到我们达到浮点极限，因为网络权重无边界增长，并且训练集上模型的预测变得越来越自信。在这种情况下，我们可以说，在训练集上错误分类误差达到零时，模型“完全适合”训练集。
    2. 如果训练过程中的梯度噪声量增加，我们发现`max_train_steps`的起始值可能需要增加。
        - 例如，如果在模型中引入了数据扩充或正则化（如丢弃）。
    3. 如果训练过程在某种程度上有所改善，那么可以减少`max_train_steps`。
        - 例如，使用更好的优化程序或更好的学习率计划。

### 3.1.1 使用学习率扫描为`max_train_steps`选择初始候选的算法

1. 该程序假设不仅可以“完美”地适应训练集，而且可以使用恒定的学习率计划表。
2. 