以下的调参策略基于两点：

1. 已经有一个训练和验证的pipeline，可以获得可用的结果
2. 拥有足够的资源，可进行有意义的调优实验，并并行运行至少几个训练任务。

## 2.1 增量调整策略

> <font color=darkred>从简单的配置开始，逐步进行改进，同时深入了解问题。确保任何改进都基于有力的证据，以避免增加不必要的复杂性。</font>

1. 最终目的是找到最优参数去最大化模型的表现
    1. 有些时候，我们需要在有限时间内最大的改进模型（比如kaggle比赛）
    2. 有些时候，我们需要不断的去提升模型（比如一些产品）
2. 原则上，我们可以通过使用算法自动搜索可能配置的整个空间来最大化性能，但这不是一个实用的选项。 
    1. 搜索空间过于巨大，在没有人为干预的情况下算法无法有效搜索
3. 多数的自动参数搜索算法，都是需要人为设置搜索空间的 
4. 最大化模型性能的最有效方法是从简单的配置开始，逐步添加功能并进行改进，同时深入了解问题。
    1. 我们在每一轮调整中使用自动搜索算法，并随着理解的增长不断更新搜索空间
5. 随着我们的探索，模型自然会表现越来越好，直到达到我们的预期的“最优”
    1. 当我们更新参数的时候，我们称之为发布
    2. 对于每一次发布，我们必须确保更改基于有力的证据，而不仅仅是基于幸运配置的随机机会，这样我们就模型调优增加不必要的复杂性

增量调整策略包括重复以下四个步骤：

1. 确定下一轮实验的适当范围目标。
2. 设计并运行一组实验，以实现这一目标
3. 从结果中了解我们能做什么
4. 考虑是否发布最新配置

## 2.2 探索与开发

## 2.2 探索与开发

> 大多数时间，我们的主要目的是深入的了解问题

1. 即使有同学认为我们花费主要的时间去优化模型在验证集的表现。但在实践中，我们花费主要时间去更加深入的理解问题，相对较少的时间贪婪地关注验证错误
    1. 换句话说，我们花费更多的时间在探索，而不是开发
2. 从长远来看，如果我们想最大限度地提高最终绩效，了解问题至关重要。将洞察力优先于短期收益可以帮助我们
    1. 避免发布不必要的版本，由于偶然的验证提升
    2. 可以找出验证错误最敏感的超参数；哪些超参数相互作用最大，因此需要一起重新调整；哪些超参数对其他变化相对不敏感，因此可以在未来的实验中固定
    3. 尝试新功能，比如对于过拟合问题可以用新的正则化方法
    4. 确定哪些功能没有作用，可以移除，以降低后续调参的复杂度
    5. 识别超参数调整的改进何时可能饱和
    6. 缩小搜索空间到最优值附近，提升调参效率
3. 当我们已经将问题了解透彻，将搜索空间减少到最优值附近，准备好进行贪婪时，我们可以只关注验证错误，即使实验不能最大程度地提供关于调优问题结构的信息


## 2.3 选择下一轮实验的目标

> 每一轮实验应该有一个目标，并且范围要最后小，这样实验才能真正朝着目标取得进展

1. 每一轮实验应该有一个目标，并且范围要最后小，这样实验才能真正朝着目标取得进展  
    1. 如果我们一口气增加多个功能，或回答多个问题，我们可能无法理清对结果的单独影响
2. 每一轮实验的目标举例
    1. 尝试pipeline的潜在提升方法（比如；增加正则化方法，预处理选择方法等）
    2. 理解模型部分超参数的影响（比如：激活函数）
    3. 贪婪的最大化验证表现


## 2.4 设计下一轮实验

> 识别科学的、多余的（有害的）和固定的超参数，并调整超参数接近实验目标。创建一系列研究以比较科学超参数的不同值，同时优化有害超参数。
选择有害超参数的搜索空间，以平衡资源成本与科学价值。

### 2.4.1 &#x1F4CC; **识别科学的、多余的（有害的）和固定的超参数（`Identifying scientific, nuisance, and fixed hyperparameters`）**

1. &#x2728; 对于给定目标，所有超参数可以是科学有效的超参数，有害的超参数或者固定的超参数
    - 科学超参数：影响我们衡量模型的指标
    - 令人讨厌的多余超参数：那些需要优化的超参数（与感兴趣参数无关，但在分析那些感兴趣参数时必须考虑的所有參數），以便公平地比较科学超参数的不同值。这类似于[有害参数的统计概念](https://en.wikipedia.org/wiki/Nuisance_parameter)
    - 固定参数：在当前轮次实验中固定其值。在比较科学超参数的不同值时，这些超参数的值不需要（或者我们不希望它们）改变（比如：随机种子`seed=42`）。
        - 通过一组实验固定某些超参数，我们必须接受从实验得出的结论可能对固定超参数的其他设置无效（比如：不同随机种子设定可能结果不同）。
2. &#x2728;举个例子，我们的目标是：模型增加隐藏层（`hidden layers`）是否会降低验证损失，那么远程层数就是一个科学超参数
    - 学习率（`learning_rate`）是一个多余参数，因为只有相同学习率下去比较才公平有效
    - 激活函数（`activation`）是一个固定超参数：如果我们在之前的实验中确定激活函数的最佳选择对模型深度不敏感，或者如果我们愿意限制我们关于隐藏层数量的结论以仅涵盖该特定选择，则激活函数可以是一个固定的超参数的激活函数，或者，如果我们准备为每个隐藏层数单独调整它，它可能是一个令人讨厌的多余超参数。
3. &#x2728;超参数是 科学有用的参数, 厌恶（多余）的参数, 固定参数不是固定不变的，依赖于实验的目标
    - 举个例子：
        - 激活函数的选择可以是科学有用的参数(在我们的问题上ReLU 和 tanh 那个是更好的选择?),
        - 激活函数的选择可以是厌恶（多余）的参数(当我们允许几种不同的激活功能时，最佳5层模型是否比6层模型好?[需要同时变换激活函数，- 否则影响结果]), 
        - 激活函数的选择可以是固定的参数 (对于ReLU激活函数的网络, 增加 batch normalization是否有帮助?).
4. &#x2728;当设计下一轮实验的时候, 需要确定我们实验目标需要调整的超参数（scientific hyperparameters）
    - 这阶段, 我们将所有的其他超参数作为厌恶（多余）的超参数 
5. &#x2728;然后, 我们从厌恶（多余）的超参数 中确定哪些设置为固定参数
    - 有了无限的资源，我们将把所有非科学的超参数作为讨厌的超参数，这样我们从实验中得出的结论就不需要对固定的超参数值提出任何警告。
    - 然而，我们尝试调整更多的厌恶（多余）的超参数, 我们就越有可能无法为每一组科学超参数充分调整它们，实验最终会得到错误的结论 
        - 如下文所述，我们可以通过增加计算预算来应对这一风险，但通常我们的最大资源预算小于调整所有非科学超参数所需的资源预算。
    - 我们选择将部分讨厌的超参数转换为固定的超参数，因为根据我们的判断，通过固定它而引入的警告比将其作为讨厌超参数包含的成本要低
        - 厌恶（多余）的超参数和scientific hyperparameter内在关联越密切, 就越难确定其值. 比如，重量衰减强度的最佳值通常取决于模型大小，因此假设重量衰减的单个特定值来比较不同的模型大小的表现并不是很合理。
6. &#x2728;虽然，参数的属性定义取决于实验目标的设置，不过可以通过以下的经验规则来确定超参数的属性 
    1. 对于**优化器的一些参数** (e.g. the learning rate, momentum, learning rate schedule parameters, Adam betas 等.), 这些参数很多都是令人厌恶的超参数，因为它们会影响其他超参数调整最终带来的效果
        - 这些参数很少会作为科学的超参数因为设置目标 "当前pipeline的最佳学习率是多少?" 并不能带来更多的业务洞察 – 并且，最佳设置很容易随下一次pipeline更改而更改
        - 虽然偶尔我们会固定一些参数，由于资源限制，或是当我们有明确的证据证明 它和科学参数没有内在关联。 我们通常应该假设优化器超参数必须单独调整，以便在科学超参数的不同设置之间进行公平的比较，因此不应该是固定的
            - 此外，我们没有先验的理由选择一个优化器超参数值而不是另一个 (e.g. 它们通常不会以任何方式影响前向传播或梯度的计算成本).
    2. 相反, **优化器的选择**是典型的scientific hyperparameter 或 fixed hyperparameter.
        1. 作为scientific hyperparameter： 如果我能实验目标是比较2个及以上优化器时(e.g. "在给定训练次数内，决定使用哪个优化器以是的验证损失最小").
        2. 作为fixed hyperparameter：  
            (1) 先前的实验使我们相信，解决问题的最佳优化器对当前的科学超参数并不敏感; 
            (2) 我们倾向于使用此优化器比较科学超参数的值，因为它的训练曲线更容易理解
            (3) 我们更喜欢使用这个优化器，因为它比其他选项使用更少的内存。
    3. **正则化技术**中的参数是典型的nuisance hyperparameters, 但是，是否使用正则化是scientific 或者 fixed hyperparameter.
        1. 举例来说, `dropout` 增加代码复杂度, 当我们想增加`dropout`时， 是否增加`dropout`是scientific hyperparameter, `dropout rate`是nuisance hyperparameter.
            - 基于本次使用，我们想要在我们的pipeline中增加`dropout``, 那么`dropout rate`在未来将作为nuisance hyperparameter。
    4. **模型结构参数**通常是scientific 或 fixed hyperparameters， 因为结构变化会影响服务和训练成本、延迟和内存需求
        - 举个例子, 网络层数是典型的scientific 或 fixed hyperparameter，它往往会对训练速度和记忆使用产生显著影响
7. &#x2728;在有些情况下, 设置nuisance and fixed hyperparameters 由 scientific hyperparameters 的值决定.
    - 举个例子, 假设我们正试图确定"Nesterov_momentum" 和 "Adam"哪一个优化器验证误差最小
        - 优化器是scientific hyperparameter, 参数值`{"Nesterov_momentum", "Adam"}`. 当选定优化器为`"Nesterov_momentum"`,  其中优化内超参数将会是 nuisance/fixed hyperparameters `{learning_rate, momentum}`;当选定优化器为`"Adam"`, 其中优化内超参数将会是 nuisance/fixed hyperparameters `{learning_rate, beta1, beta2, epsilon}`.
    - 仅对科学超参数的某些值存在的超参数称为条件超参数
    - 我们不应该仅仅因为两个条件超参数具有相同的名称就假设它们是相同的！在上面的示例中， learning_rate的条件超参数是优化器=“Nesterov_motoment”与优化器=“Adam”的不同超参数。它在两种算法中的作用相似（虽然不完全相同），但在每种优化器中工作良好的值范围通常不同几个数量级。

### 2.4.2 &#x1F4CC; **构建一系列的学习器**

1. &#x2728;一旦我们确定了 scientific 和 nuisance hyperparameters，我们就设计一项“研究”或一系列研究，以朝着实验目标取得进展。
    1. 一个研究指定了一组用于后续分析的超参数配置，每一个配置称为实验（`trial`).
    2. 创建一个study 典型的 超参数选择，这在不同的试验中会有所不同, choosing 设置参数搜索空间(`"search space"`), 选择实验的次数, 选择一个自动搜索算法，从搜索空间中抽取那么多试验样本。或者，我们可以通过手动指定一组超参数配置来创建研究
2. &#x2728;studies目的是用不同的 scientific hyperparameters 运行pipeline, 并且，同时不断优化 (or "optimizing over") nuisance hyperparameters，从而使科学超参数的不同值之间的比较尽可能公平
3. &#x2728;在最简单的情况下, 我们将对科学参数的每种配置进行单独的研究，其中每项研究都会对 nuisance hyperparameters 进行调整。
    1. 举个例子, 我们的目标是在 Nesterov momentum 和 Adam 中选择最优优化器, 构建一个study，优化器="Nesterov_momentum"，然后 nuisance hyperparameters就是`{learning_rate, momentum}`, 构建另一个study，优化器="Adam"，然后 nuisance hyperparameters就是`{learning_rate, beta1, beta2, epsilon}` 。我们比较连个优化器 optimizers，通过实验选择对应优化器参数，拿出最好的进行比较
    2. 我们可以使用任何无梯度优化的算法, 包括：`Bayesian optimization`或 `evolutionary algorithms`, 去优化上述nuisance hyperparameters, 虽然，我们倾向于使用准随机搜索(`quasi-random search`) 在调整的探索阶段，因为它在这种环境中具有多种优势，探索结束后，如果最先进的贝叶斯优化软件可用，这是我们的首选。
4. &#x2728;在更复杂的情况下，我们希望比较大量的科学超参数值，并且进行如此多的独立研究是不切实际的, 我们可以将科学参数包含在与讨厌的超参数相同的搜索空间中，并使用搜索算法在单个研究中对科学和讨厌的超参数值进行采样.
    1. 当采用这种方法时，条件超参数可能会导致问题，因为很难指定搜索空间，除非对于科学超参数的所有值，讨厌的超参数集合是相同的。
    2. 在这个例子中, 我们更倾向于使用准随机搜索(`quasi-random search`) 而不是更花哨的黑箱优化工具, 因为它确保我们获得科学超参数值的相对均匀的采样。无论搜索算法如何，我们都需要确保它以某种方式统一搜索科学参数.


### 2.4.3 &#x1F4CC;  **平衡更多信息和现实限制下设计实验**

1. &#x2728;在设计研究或研究序列时，我们需要分配有限的预算，以充分实现以下三个需求
    - 比较足够不同的`scientific hyperparameters`数值。
    - 在足够大的搜索空间上调整`nuisance hyperparameters`。
    - 足够密集地采样`nuisance hyperparameters`的搜索空间。
2. &#x2728;我们越能实现以上三个需求，就能从实验中获得更多的洞见
    - 比较尽可能多的科学超参数值可以扩大我们从实验中获得的见解的范围
    - 包括尽可能多的`nuisance hyperparameters`并允许每个`nuisance hyperparameters`在尽可能宽的范围内变化，增加了我们的信心，即`nuisance hyperparameters`的“良好”值存在于科学超参数的每个配置的搜索空间中。
        - 否则，我们可能会通过不搜索有`nuisance hyperparameters`空间的可能区域来对科学超参数的值进行不公平的比较，在这些区域中，科学参数的某些值可能存在更好的值。
3. &#x2728;尽可能密集地对`nuisance hyperparameters`的搜索空间进行采样增加了我们的信心，即搜索过程将找到恰好存在于我们的搜索空间中的`nuisance hyperparameters`的任何良好设置。
4. &#x2728;不幸的是，在这三个维度中的任何一个维度上的改进都需要增加试验次数，从而增加资源成本，或者在其他维度中找到节省资源的方法.
    - <font color=darkred>每个问题都有自己的特点和计算限制，因此如何在这三个需求之间分配资源需要一定程度的领域知识。</font>
    - 在进行一项研究后，我们总是试图了解该研究是否足够好地调整了`nuisance hyperparameters`（即，足够广泛地搜索了足够大的空间），以公平地比较科学超参数。

### 2.4.4 &#x1F4CC; **从实验结果中提取见解**

> 除了努力实现每组实验的原始科学目标外，还应仔细检查其他问题的清单，如果发现问题，则修改实验并重新进行。

1. &#x2728;最后，每组实验都有一个特定的目标，我们希望评估实验为实现这个目标提供的证据。
    - 然而，如果我们提出正确的问题，我们通常会发现在给定的一组实验朝着其原始目标取得很大进展之前需要纠正的一些问题。
        - 如果我们不问这些问题，我们可能会得出错误的结论.
    - 由于运行实验可能会很昂贵, 因此我们也希望借此机会从每组实验中提取其他有用的见解（对需要解决的问题有更多的了解）, 即使这些见解与当前目标并不立即相关。
2. &#x2728;在分析一组给定的实验以实现其原始目标之前，我们还需要回答以下附加问题：
   1. 搜索空间是否足够大
      - 如果研究的最佳点在一个或多个维度上接近搜索空间的边界,则搜索可能不够宽. 在这种情况下，我们应该使用扩展的搜索空间运行另一项研究。
   2. 我们从搜索空间中采样了足够多的点吗?
      - 如果没有，在调整目标中多跑几步或少跑几步。
   3. 每项研究中哪部分试验不可行 (i.e. 出现分歧的试验，损失值非常糟糕或者因为违反了某些隐含约束而根本无法运行)?
      - 当研究中有很大一部分点不可行时，我们应该尝试调整搜索空间，以避免对这些点进行采样，这有时需要对搜索空间进行重新参数化。
      - 在某些情况下，大量不可行点可能表明训练代码中存在错误。
   4. 模型是否存在优化问题？
   5. 我们可以从最佳试验的训练曲线中学到什么？
      - 例如，最好的试验是否具有与有问题的过度拟合一致的训练曲线？
3. &#x2728;如有必要，根据上述问题的答案，完善最近的研究（或一组研究），以改善搜索空间和/或抽样更多试验，或采取其他纠正措施。
4. &#x2728;一旦我们回答了上述问题，我们就可以继续评估实验为我们的原始目标提供的证据（例如，评估改变是否有用）。

#### 识别错误的搜索空间边界

1. 如果从搜索空间采样的最佳点接近其边界，则搜索空间是可疑的。如果我们朝这个方向扩大搜索范围，我们可能会找到更好的点。
2. 为了检查搜索空间边界, 我们喜欢在我们所称的基本超参数轴图上绘制完成的试验，其中我们绘制了验证目标值与超参数之一（例如学习率）之间的关系。图上的每个点对应于一次试验。
    - 每次试验的验证目标值通常应为训练过程中达到的最佳值。
3. 图1中的图表显示了相对于初始学习率的错误率（越低越好）
4. 如果最佳点朝向搜索空间的边缘聚集（在某个维度上），则可能需要扩展搜索空间边界，直到最佳观察点不再靠近边界。
5. 通常，一项研究将包括“不可行”的试验，这些试验会产生分歧或得到非常糟糕的结果（在上面的图中用红色X标记）。
   - 如果所有试验对于大于某个阈值的学习率都是不可行的，并且如果表现最好的试验在该区域的边缘具有学习率，则模型可能会受到稳定性问题的影响，无法访问更高的学习率。

#### 在搜索空间中采样不足

1. 通常，很难知道搜索空间是否采样足够密集. 🤖
2. 最好进行更多试验，但是会带来明显的运行成本
3. 因为很难知道搜索空间是否采样足够密集, 我们通常会对我们能负担得起的东西进行采样，并通过反复查看各种超参数轴图来校准我们的直觉信心，并试图了解搜索空间的“好”区域中有多少点


#### 检查训练曲线

> 检查训练曲线是识别常见故障模式的简单方法，可以帮助我们确定下一步要采取的行动的优先级

1. &#x2728; 尽管在许多情况下，我们的实验的主要目标只需要考虑每个实验的验证错误，但当将每个实验减少到单个数字时，我们必须小心，因为这可能会隐藏关于表面下发生的事情的重要细节。
2. &#x2728; 对于每一项研究，我们总是查看至少最好的几个试验的训练曲线（训练误差和验证误差在训练期间与训练步骤的对比图）。
3. &#x2728; 即使这不是解决主要实验目标所必需的，检查训练曲线也是识别常见故障模式的一种简单方法，可以帮助我们确定下一步要采取的行动的优先级。
4. &#x2728; 在检查训练曲线时，我们对以下问题感兴趣。
    1) 是否有任何试验显示过度拟合问题？
        - 当验证错误在训练过程中的某一点开始增加时，就会出现问题性的过度拟合。
        - 在实验设置中，我们通过为每个科学超参数设置选择“最佳”试验来优化去除讨厌的超参数，我们应该检查与我们正在比较的科学超参数的设置相对应的至少每个最佳试验中是否存在问题的过拟合。
            a. 如果出现过拟合，我们通常希望在比较科学超参数的值之前，使用额外的正则化技术重新运行实验和/或更好地调整现有的正则化参数。
            b. 使用增加最小代码复杂度或额外计算（例如，丢弃、标签平滑、权重衰减）的常用正则化技术，减少过度拟合通常很简单。
            c. 例如，如果科学超参数是“隐藏层的数量”，并且使用最大数量的隐藏层的最佳试验显示出有问题的过度拟合，那么我们通常倾向于使用额外的正则化来再次尝试，而不是立即选择较小数量的隐层。
            d. 即使没有一项“最佳”试验表现出有问题的过度拟合，但如果在任何一项试验中出现，仍可能存在问题。
                - 选择最佳试验可以抑制出现问题的过度拟合配置，而有利于不存在问题的配置。换言之，它将有利于正则化更加多的配置。
                - 然而，任何让训练变得更糟糕的事情都可以称为正则，即使不是故意的。例如，选择较小的学习率可以通过阻碍优化过程来规范训练，但我们通常不希望这样选择学习率。
                - 因此，我们必须意识到，每种科学超参数设置的“最佳”试验可能以有利于某些科学或令人讨厌的超参数的“坏”值的方式选择。
    2) 训练中是否存在较大的逐步差异或训练后期的验证错误？
        - 如果是这样，这可能会干扰我们比较科学超参数的不同值的能力（因为每次试验都随机以“幸运”或“不幸运”的步骤结束），以及影响模型的实际效果（由于生产环境中可能不会以与研究中相同的“幸运”步骤结束）。
        - 分步方差最可能的原因是批次方差（从每个批次的训练集中随机抽样样本）、验证集过小以及在训练后期使用过高的学习率。
        - 可能的补救措施包括增加批量、获得更多验证数据、使用学习率衰减或使用[Polyak](https://paperswithcode.com/method/polyak-averaging)平均。
    3) 训练结束后，试训是否仍在改善
        - 如果是，那么我们处于“计算受限”状态，我们可以增加训练步骤或者更改learning rate schedule
    4) 在最后的训练步骤之前，训练和验证集的表现是否已经饱和？
        - 如果是，那么我们处于“不受计算约束”状态，并且我们可能能够减少训练步骤的数量。
5. &#x2728; 虽然我们不能列举所有这些，但通过检查训练曲线，可以发现许多其他行为（例如，训练过程中训练损失增加通常表明训练管道中存在错误）

#### 使用隔离图检测更改是否有用

1. 通常，一组实验的目的是比较科学超参数的不同值
2. 隔离图是基本超参数轴图的特殊情况。隔离图上的每个点对应于某些（或所有）有害超参数的最佳试验性能。
3. 隔离图可以更轻松地在科学超参数的不同值之间进行同类比较
4. 例如，图2显示了在ImageNet上训练的ResNet-50的特定配置产生最佳验证性能的权重衰减值。
   - 如果我们的目标是确定是否包含体重衰减，那么我们会将此图中的最佳点与无体重衰减的基线进行比较。为了公平比较，基准线的学习率也应该调整得同样好。
5. 当我们通过（准）随机搜索生成数据并考虑隔离图的连续超参数时，我们可以通过对基本超参数轴图的x轴值进行分段并在由桶定义的每个垂直切片中进行最佳试验来近似隔离图。

#### 自动化生成用用的绘图

1. 生成图的工作量越大，我们就越不可能尽可能多地查看它们，因此我们有必要建立基础设施，以自动生成尽可能多的图。
2. 至少，我们会为实验中变化的所有超参数自动生成基本超参数轴图。
3. 此外，我们为所有试验自动生成训练曲线，并尽可能容易地找到每个研究中最好的几个试验并检查其训练曲线。
4. 我们可以添加许多其他可能的绘图和可视化效果，这些都很有用。尽管上面描述的是一个很好的起点，套用杰弗里·辛顿的话，“每次你策划新的东西，你都会学到新的东西。”

## 2.5 确定是否更改训练pipeline或者采用超参数配置

> 在决定是否对我们的模型或训练程序进行更改或采用新的超参数配置时，我们需要了解结果的不同来源。

1. &#x1F4CC; 当我们试图改进我们的模型时，我们可能会观察到，与现有配置相比，特定的候选更改最初实现了更好的验证错误，但发现重复实验后，没有一致的优势。非正式地，我们可以将可能导致这种不一致结果的最重要的变异源分为以下大类：
    1. 训练差异、再训练差异或试验差异：使用相同的配置参数进行训练，但是使用不同的随机种子
        - 例如, 不同的随机初始化，不同的数据乱序, 不同的`dropout masks`, 部分数据扩增操作， 以及并行算术运算的排序,这些都是有可能带来试验误差的。
    2. 超参数搜索方差或研究方差：由我们选择超参数的程序导致的结果变化
        - 例如，我们可以对特定搜索空间运行相同的实验，但使用两个不同的种子进行准随机搜索，最终选择不同的超参数值
    3. 数据收集和抽样方差：任何类型的随机分为训练、验证和测试数据的方差，或更一般的训练数据生成过程产生的方差。
2. &#x1F4CC; 使用严格的统计测试对有限验证集上估计的验证错误率进行比较是很好的，但通常仅试验方差就可以在使用相同超参数设置的两个不同训练模型之间产生统计显著差异。
3. &#x1F4CC; 当我们试图得出超出超参数空间中单个点水平的结论时，我们最关心的是研究方差。
    - 研究方差取决于试验的数量和搜索空间，我们已经看到了试验方差大于试验方差的情况以及试验方差小得多的情况。
4. &#x1F4CC; 因此，在采用候选变更之前，考虑进行N次最佳试验，以表征逐次试验方差。
    - 通常，我们只需在`pipeline`发生重大变化后重新描述试验方差即可，但在某些应用中，我们可能需要更新的估计。
    - 在其他应用中，表征试验方差的成本太高，不值得。
5. &#x1F4CC; 最后，尽管我们只想采用能够产生真正改进的更改（包括新的超参数配置），但要求完全确定某些东西有帮助也不是正确的答案。
6. &#x1F4CC; 因此，如果一个新的超参数点（或其他变化）得到了比`baseline`更好的结果（尽可能考虑到新点和基线的再训练方差），那么我们可能应该将其作为未来比较的`new baseline`
    - 然而，我们应<font color=darkred>该只采用那些带来的改进超过了它们增加的复杂性的更改</font>

## 2.6 勘探结束后

> 一旦我们完成了对良好搜索空间的探索，并决定了哪些超参数甚至应该被调整，贝叶斯优化工具就是一个令人信服的选项。

1. &#x1F4CC; 在某个时刻，我们的优先项将从学习更多关于调优问题转向生成一个最佳配置以启动或以其他方式使用。
2. &#x1F4CC; 在这个时刻，需要重新定义搜索空间，该空间舒适地包含最佳观察试验周围的局部区域，并且已经充分采样
3. &#x1F4CC; 我们的探索工作应该已经揭示了要调整的最基本的超参数（以及它们的合理范围），我们可以使用尽可能大的调整预算来构建最终自动调整研究的搜索空间
4. &#x1F4CC; 由于我们不再关心最大限度地了解调谐问题，准随机搜索的许多优点不再适用，应使用贝叶斯优化工具自动找到最佳超参数配置。
    - 如果搜索空间包含非平凡数量的发散点（获得NaN训练损失或甚至训练损失比平均值差很多标准差的点），那么使用黑箱优化工具来正确处理发散的试验是非常重要的（请参阅具有未知约束的贝叶斯优化以获得处理此问题的最佳方法）。
5. &#x1F4CC; 此时，我们还应考虑检查测试集的性能
    - 原则上，我们甚至可以将验证集折叠到训练集中，并通过贝叶斯优化重新训练找到的最佳配置。然而，只有在未来不会有这种特定工作量的发布（例如一次性Kaggle竞赛）时，这才是合适的。

